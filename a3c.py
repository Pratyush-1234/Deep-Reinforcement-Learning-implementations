# -*- coding: utf-8 -*-
"""A3C.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dtfLHcWZxrsSs3ffrNGM8HKyvHHiFPFg
"""

import gym
import torch as T
import torch.multiprocessing as mp
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
import pybullet_envs
from gym import wrappers
import matplotlib.pyplot as plt
# explicitly tell Torch that we want to share memory among the
# various networks, and initialize the appropriate variables
class SharedAdam(T.optim.Adam):
  def __init__(self,params,lr=1e-3,betas=(0.9,0.99),eps=1e-8,weight_decay=0):
    super(SharedAdam,self).__init__(params,lr=lr,betas=betas,eps=eps,weight_decay=weight_decay)
    for group in self.param_groups:
      for p in group['params']:
        state=self.state[p]
        state['step']= 0
        state['exp_avg']=T.zeros_like(p.data)
        state['exp_avg_sq']=T.zeros_like(p.data)
        state['exp_avg'].share_memory_()
        state['exp_avg_sq'].share_memory_()
# implement neural nets, feed forward, action selection,
# loss calculation, discount return calculation, and memory here
class ActorCritic(nn.Module):
  def __init__(self,input_dims,n_actions,gamma=0.99):
    super(ActorCritic,self).__init__()
    self.gamma=gamma
    self.pi1=nn.Linear(*input_dims,128)
    self.v1=nn.Linear(*input_dims,128)
    self.pi=nn.Linear(128,n_actions)
    self.v=nn.Linear(128,1)
    self.rewards = []
    self.actions = []
    self.states = []
  def remember(self,state,action,reward):
    self.states.append(state)
    self.actions.append(action)
    self.rewards.append(reward)
  def clear_memory(self):
    self.states=[]
    self.actions=[]
    self.rewards=[]
  def forward(self,state):
    pi1=F.relu(self.pi1(state))
    v1=F.relu(self.v1(state))
    pi=self.pi(pi1)
    v=self.v(v1)
    return pi,v
  def calc_R(self,done):
    states=T.tensor(np.array(self.states),dtype=T.float)
    _,v=self.forward(states)
    R=v[-1]*(1-int(done))
    batch_return=[]
    for reward in self.rewards[::-1]:
      R=reward+self.gamma*R
      batch_return.append(R)
    batch_return.reverse()
    batch_return=T.tensor(batch_return,dtype=T.float)
    return batch_return
  def calc_loss(self,done):
    states = T.tensor(np.array(self.states), dtype=T.float)
    actions = T.tensor(np.array(self.actions), dtype=T.float)
    returns=self.calc_R(done)
    pi,values=self.forward(states)
    values=values.squeeze()# very important
    critic_loss=(returns-values)**2
    probs=T.softmax(pi,dim=1)
    dist=Categorical(probs)
    log_probs=dist.log_prob(actions)
    actor_loss=-log_probs*(returns-values)
    total_loss=(critic_loss+actor_loss).mean()
    return total_loss
  def choose_action(self,observation):
    state=T.tensor(np.array([observation]),dtype=T.float)
    pi,v=self.forward(state)
    probs=T.softmax(pi,dim=1)
    dist=Categorical(probs)
    action=dist.sample().numpy()[0]
    return action
# pass in the global optimizer, global actor critic, environment details
# reset environment and agent’s memory
# while not done
# play game according to each agent’s policy
# if t_step % T_MAX or terminal step
# use sampled returns and actions to calculate losses
# upload gradients to global optimizer
# step optimizer
# download parameters from global agent to local agent
# wipe memory
class Agent(mp.Process):
  def __init__(self,global_actor_critic,optimizer,input_dims,n_actions,gamma,lr,name,global_ep_idx,env):
    super(Agent, self).__init__()
    self.local_actor_critic = ActorCritic(input_dims, n_actions, gamma)
    self.global_actor_critic = global_actor_critic
    self.name = 'w%02i' % name
    self.episode_idx=global_ep_idx
    self.env=env
    self.optimizer = optimizer
    # declare the appropriate global variables
    # instantiate our worker agents
    # threading process

  def run(self):
    t_step=1
    score_history = []
    avg_score_history = []
    observations=[]
    actions=[]
    path_length=[]
    while self.episode_idx.value<N_GAMES:
      done=False
      observation=self.env.reset()
      score=0
      self.local_actor_critic.clear_memory()
      while not done:
        action=self.local_actor_critic.choose_action(observation)
        observation_, reward, done, info = self.env.step(action)
        score+=reward
        score_history.append(score)
        avg_score = np.mean(score_history[-100:])
        avg_score_history.append(avg_score)
        self.local_actor_critic.remember(observation, action, reward)
        if t_step % T_MAX == 0 or done:
          loss=self.local_actor_critic.calc_loss(done)
          self.optimizer.zero_grad()
          loss.backward()
          for local_param,global_param in zip(self.local_actor_critic.parameters(),self.global_actor_critic.parameters()):
            global_param._grad = local_param.grad
            self.optimizer.step()
            self.local_actor_critic.load_state_dict(self.global_actor_critic.state_dict())
            self.local_actor_critic.clear_memory()
        t_step += 1
        observation = observation_
        observations.append(observation)
        actions.append(action)
      print(path_length)
      path_length.append(info["path_length"])
      with self.episode_idx.get_lock():
        self.episode_idx.value+=1
      print(self.name, 'episode ', self.episode_idx.value, 'reward %.1f' % score)
    if self.name == 'w00':
        plt.plot(range(len(avg_score_history)), avg_score_history)
        plt.xlabel('Episode')
        plt.ylabel('Average Score')
        plt.title('Average Score Over Time')
        plt.show()
        plt.plot(range(len(path_length)),path_length)
        plt.xlabel('Episode')
        plt.ylabel('path length')
        plt.title('path length Over Time')
        plt.show()
        plt.plot(actions)
        plt.xlabel('Time Step')
        plt.ylabel('Actions')
        plt.title('Actions Over Time')
        plt.show()

        plt.plot(observations)
        plt.xlabel('Time Step')
        plt.ylabel('Observations')
        plt.title('Observations Over Time')
        plt.show()
        # Testing phase
        n_test_games = 10
        test_scores = []
        path_length = []
        reached_goal = []
        for i in range(n_test_games):
            observation = env.reset()
            done = False
            score = 0
            while not done:
                action = self.local_actor_critic.choose_action(observation)
                observation_, reward, done, info = env.step(action)
                score += reward
                observation = observation_
            print(path_length)
            path_length.append(info["path_length"])
            reached_goal.append(info["reached_goal"])
            test_scores.append(score)
            print('Test episode ', i, 'score %.1f' % score)

        avg_test_score = np.mean(test_scores)
        print('Average test score: %.1f' % avg_test_score)

        # Plot test scores
        plt.plot(range(n_test_games), test_scores)
        plt.xlabel('Test Episode')
        plt.ylabel('Score')
        plt.title('Test Scores Over Episodes')
        plt.show()
        plt.plot(range(n_test_games),path_length)
        plt.xlabel('Episode')
        plt.ylabel('path length')
        plt.title('Test path length Over Time')
        plt.show()
        plt.plot(np.cumsum(reached_goal) / np.arange(1, n_test_games + 1))
        plt.xlabel('Episode')
        plt.ylabel('Proportion of Episodes Reached Goal')
        plt.title('Proportion of Episodes Reached Goal Over Time')
        plt.show()

import gym
from gym import spaces
import numpy as np
import matplotlib.pyplot as plt

class BoxNavigationEnv(gym.Env):
    def __init__(self):
        super(BoxNavigationEnv, self).__init__()

        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        self.goal_position = np.array([0.9, 0.9])
        self.initial_position = np.array([0.1, 0.1])
        self.obstacles = [
            {'position': np.array([0.5, 0.5]), 'size': 0.1},
            {'position': np.array([0.2, 0.8]), 'size': 0.1},
        ]

        self.state = self.initial_position.copy()
        self.path_length = 0.0

    def reset(self):
        """Reset the environment to the initial state with some randomness."""
        self.state = self.initial_position + np.random.uniform(-0.05, 0.05, size=2)
        self.state = np.clip(self.state, 0, 1)
        self.path_length = 0.0  # Reset path length at the start of a new episode
        return self.state

    def step(self, action):
        previous_state = self.state.copy()
        # Clip the action to stay within bounds and scale it
        action = np.clip(action, -1, 1)
        self.state = np.clip(self.state + action * 0.1, 0, 1)

        # Calculate the distance traveled in this step
        step_distance = np.linalg.norm(self.state - previous_state)
        self.path_length += step_distance

        # Calculate distance to goal
        distance_to_goal = np.linalg.norm(self.state - self.goal_position)

        # Check if the agent has reached the goal
        if distance_to_goal < 0.05:
            reward = 100  # Reward for reaching the goal
            done = True
            info = {"truncated": False, "path_length": self.path_length, "reached_goal": True}
            return self.reset(), reward, done, info

        # Check for collisions with obstacles
        for obstacle in self.obstacles:
            if np.linalg.norm(self.state - obstacle['position']) < obstacle['size']:
                reward = -100  # Penalty for hitting an obstacle
                done = True
                info = {"truncated": False, "path_length": self.path_length, "reached_goal": False}
                return self.reset(), reward, done, info

        # Reward shaping: provide intermediate rewards
        reward = -1  # Small penalty for each step taken
        reward += -distance_to_goal  # Encourage getting closer to the goal

        # Additional penalty for being close to obstacles
        for obstacle in self.obstacles:
            obstacle_distance = np.linalg.norm(self.state - obstacle['position'])
            if obstacle_distance < obstacle['size'] + 0.1:
                reward += -10 * (0.2 - obstacle_distance)  # Larger penalty for being very close to obstacles

        done = False
        info = {"truncated": False, "path_length": self.path_length, "reached_goal": False}

        return self.state, reward, done, info

if __name__ == '__main__':
    lr = 1e-4
    env = BoxNavigationEnv()
    n_actions = env.action_space.shape[0]
    input_dims = env.observation_space.shape
    N_GAMES = 3000
    T_MAX = 5
    global_actor_critic = ActorCritic(input_dims, n_actions)
    global_actor_critic.share_memory()
    optim = SharedAdam(global_actor_critic.parameters(), lr=lr,
                        betas=(0.92, 0.999))
    global_ep = mp.Value('i', 0)

    workers = [Agent(global_actor_critic,
                    optim,
                    input_dims,
                    n_actions,
                    gamma=0.99,
                    lr=lr,
                    name=i,
                    global_ep_idx=global_ep,
                     env=env) for i in range(mp.cpu_count())]
    [w.start() for w in workers]
    [w.join() for w in workers]